import os
import json
from time import sleep

from click import argument, group, option
# from camelot import read_pdf
from numpy import percentile
from transformers import pipeline, AutoTokenizer
from tqdm import tqdm
from pathlib import Path
from docx.api import Document

from .util import unpack, normalize_spaces, is_number
from .Cell import Cell


@group()
def main():
    pass


MAX_LENGTH = 512


class TableTranslator:
    def __init__(self, model: str = 'Helsinki-NLP/opus-mt-ru-en'):
        self.pipeline = pipeline('translation', model = model, framework = 'pt', device = 'cuda', max_length = MAX_LENGTH)
        self.tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-ru-en")

    def count(self, text: str):
        return len(self.tokenizer(text)['input_ids'])

    def _split(self, text: str):
        text_components = text.split(' ')
        middle_index = len(text_components) // 2

        chunks = []

        lhs = ' '.join(text_components[:middle_index])
        rhs = ' '.join(text_components[middle_index:])

        def push(half: str):
            if self.count(half) > MAX_LENGTH:
                for chunk in self._split(half):
                    chunks.append(chunk)
            else:
                chunks.append(half)

        push(lhs)
        push(rhs)

        # print(text)
        # print('==')
        # print(lhs, self.count(lhs))
        # print('--')
        # print(rhs, self.count(rhs))

        return chunks

    def translate_row(self, row: list[str]):
        merge_flags = []
        row_chunks = []

        for item in row:
            if self.count(item) > MAX_LENGTH:
                chunks = self._split(item)

                for chunk in chunks:
                    row_chunks.append(chunk)

                merge_flags.append(False)
                merge_flags.extend([True for _ in range(len(chunks) - 1)])
            else:
                row_chunks.append(item)
                merge_flags.append(False)

        translated_texts = [cell['translation_text'] for cell in self.pipeline(row_chunks)]
        final_texts = []

        last_text = None

        for text, should_merge_with_last in zip(translated_texts, merge_flags):
            if last_text is not None:
                if should_merge_with_last:
                    last_text = ' '.join((last_text, text))
                    print(last_text)
                else:
                    final_texts.append(last_text)
                    last_text = text
            else:
                last_text = text

        final_texts.append(last_text)

        print(len(final_texts), len(translated_texts))

        return final_texts

    def translate(self, table: list[list[str]]):
        translated_table = []

        for row in tqdm(table, desc = 'Translating table'):
            translated_table.append(self.translate_row(row))

        return translated_table


@main.command(name = 'unpack')
@argument('source', type = str)
@argument('destination', type = str, required = False)
def unpack_(source: str, destination: str):
    if destination is None:
        destination = os.path.join('assets', Path(source).stem)

    unpack(source, destination)


@main.command()
@argument('source', type = str)
@argument('destination', type = str)
@option('--first-n', '-n', type = int, required = False)
def translate(source: str, destination: str, first_n: int):
    print('Collecting texts...')

    if not os.path.isdir(destination):
        os.makedirs(destination)

    texts = []
    tables = []

    source_files = os.listdir(source)

    if first_n is not None:
        source_files = source_files[:first_n]

    for source_file in source_files:
        with open(os.path.join(source, source_file), 'r') as file:
            table = json.load(file)

            for row in table['rows']:
                for cell in row:
                    if (text := cell.get('text')) is not None and len(text) > 0 and not is_number(text):
                        texts.append(text)
                        cell['_requires-translation'] = True
                    else:
                        cell['_requires-translation'] = False

            table['_filename'] = source_file

            tables.append(table)

    # print(len(texts))
    # print(tables[0])

    print('Translating...')

    translator = TableTranslator()

    translated_texts = translator.translate_row(texts)

    with open('assets/new-specs/translation-log.txt', 'w') as file:
        for text, translated_text in zip(texts, translated_texts):
            file.write(f'{text} ðŸ”´ {translated_text}\n')

    offset = 0

    for table in tables:
        for row in table['rows']:
            for cell in row:
                if cell.pop('_requires-translation'):
                    cell['text'] = translated_texts[offset]
                    offset += 1

        filename = table.pop('_filename')

        with open(os.path.join(destination, filename), 'w') as file:
            json.dump(table, file, indent = 2, ensure_ascii = False)


@main.command()
@argument('source', type = str)
@argument('destination', type = str)
def parse(source: str, destination: str):
    for source_file in tqdm(os.listdir(source)):
        document = Document(os.path.join(source, source_file))

        if not os.path.isdir(destination):
            os.makedirs(destination)

        for i, table in enumerate(document.tables):
            parsed_rows = []
            destination_file = os.path.join(destination, f'{Path(source_file).stem}.{i}'.replace(' ', '_')) + '.json'

            if os.path.isfile(destination_file):
                continue

            try:
                rows = table.rows
            except:
                print(f'Error when parsing table {i} from file {source_file}. Skipping...')
                continue

            skip_table = False

            for row in rows:
                parsed_cells = []

                try:
                    cells = row.cells
                except:
                    print(f'Error when parsing table {i} from file {source_file}. Skipping...')
                    skip_table = True
                    break

                for cell in cells:
                    # print(dir(cell))
                    # print(cell._element.xml)

                    parsed_cells.append(Cell(normalize_spaces(cell.text)))

                try:
                    parsed_rows.append(Cell.merge_horizontally(parsed_cells))
                except:
                    print(f'Error when merging horizontally on file {source_file}')

            if skip_table:
                continue

            try:
                parsed_rows = Cell.merge_vertically(parsed_rows)
            except:
                print(f'Error when merging vertically on file {source_file}')

            # for row in parsed_rows:
            #     print(row)

            # print(source)

            # print(Cell.serialize_rows(parsed_rows))

            # i = 0


            with open(destination_file, 'w') as file:
                json.dump(Cell.serialize_rows(parsed_rows), file, indent = 2, ensure_ascii = False)


@main.command()
@argument('path', type = str)
@option('--pages', '-p', type = str, default = 'all')
def extract_tables(path: str, pages: str):
    tables = read_pdf(path, pages = pages, flavor = 'lattice')

    # 1. Split rows into cells

    rows = []
    row_lengths = []
    max_source_row_length = 1

    for row in tables[0].data:
        if (source_row_length := len(row)) > max_source_row_length:
            max_source_row_length = source_row_length

        updated_row = row[0].split('\n')

        row_lengths.append(len(updated_row))

        rows.append(updated_row)

    if max_source_row_length < 2:
        # 2. Chose a reasonable number columns

        n_cols = int(percentile(row_lengths, 75))

        # 3. Fold the table rows

        table = []

        for row in rows:
            i = 0
            for cell in row[n_cols:]:
                cell_index = i % n_cols

                row[cell_index] += f' {cell}'
                i += 1

            table.append(row[:n_cols])
    else:
        table = tables[0].data

    for row in table:
        print(row)

    print('Question:')
    question = input('> ')

    print('Answer:')
    answer = input('> ')

    translator = TableTranslator()

    question, answer = translator.translate_row([question, answer])

    # 4. Translate to english

    table = translator.translate(table)

    for row in table:
        print(row)

    print(question)
    print(answer)

    # 5. Make data sample

    sample = {
        "feta_id": 2206,
        "table_source_json": "totto_source/dev_json/example-2205.json",
        "page_wikipedia_url": "http://en.wikipedia.org/wiki/Shagun_Sharma",
        "table_page_title": "Shagun Sharma",
        "table_section_title": "Television",
        "table_array": table,
        "highlighted_cell_ids": [[6, 0], [6, 1], [6, 2], [7, 0], [7, 1], [7, 2], [8, 0], [8, 1], [8, 2]],
        "question": question,
        "answer": answer,
        "source": "mturk-approved"
    }

    print(sample)


if __name__ == '__main__':
    main()
